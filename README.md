# ğŸ¬ Help Frida - Movie Data Processing Pipeline

![Dataset Status](https://img.shields.io/badge/Dataset-Production_Ready-brightgreen)
![Movies Processed](https://img.shields.io/badge/Movies-2372_Records-blue)
![Unique Films](https://img.shields.io/badge/Films-885_Unique-blue)
![Quality Score](https://img.shields.io/badge/Quality-100%2F100-green)
![Completeness](https://img.shields.io/badge/Completeness-98.7%25-green)
![Status](https://img.shields.io/badge/Status-Production-brightgreen)

## ğŸ¯ **SALIDAS FINALES DEL PROYECTO**

### ğŸ“„ **ARCHIVOS FINALES DE PRODUCCIÃ“N**

#### â­ **Archivo Principal: `final_check/MOVIES_MASTER_FINAL.csv`**
**Dataset definitivo certificado para `EXTERNAL_SOURCES.CINEPOLIS.MOVIES_MASTER`**

| MÃ©trica | Valor | UbicaciÃ³n | Status |
|---------|-------|------------|--------|
| **Registros** | 2,372 | `final_check/MOVIES_MASTER_FINAL.csv` | âœ… |
| **Columnas** | 16 | Estructura completa | âœ… |
| **PelÃ­culas Ãšnicas** | 885 | Mapeo 1:1 perfecto | âœ… |
| **Completitud** | 98.7% | 37,457/37,952 celdas | âœ… |
| **Calidad** | 100.0/100 | Certificado para producciÃ³n | âœ… |

#### ğŸ“ **Archivo Especializado: `final_check/descripcion.csv`**
**Mapeo Ãºnico NOMBRE_UNICO â†’ DESCRIPCION para anÃ¡lisis de contenido**

| MÃ©trica | Valor | UbicaciÃ³n | Uso |
|---------|-------|------------|-----|
| **Registros Ãšnicos** | 885 | `final_check/descripcion.csv` | AnÃ¡lisis de texto |
| **Columnas** | 2 (NOMBRE_UNICO, DESCRIPCION) | Sin duplicados | Text mining |
| **Completitud** | 100% | Todas las descripciones | BÃºsquedas temÃ¡ticas |

#### ğŸ”’ **Certificaciones de Calidad:**
- âœ… Mapeo 1:1 perfecto (NOMBRE_UNICO â†” MOVIE_ID)
- âœ… Coherencia horizontal (CATEGORIA = CATEGORIA_CINEPOLIS)
- âœ… Coherencia vertical (metadatos consistentes por pelÃ­cula)
- âœ… Formato profesional (69 categorÃ­as de una palabra)
- âœ… Cero registros duplicados
- âœ… ValidaciÃ³n completa (10/10 verificaciones)

## ğŸ—ï¸ **Processing Pipeline Architecture**

This repository implements a sophisticated 4-stage movie data processing pipeline:

```mermaid
graph LR
    A["Raw Transactions<br/>17M+ records"] --> B["Movie Standardizer<br/>Deduplication & Normalization"]
    B --> C["Parche Output<br/>Web Scraping & Enrichment"]
    C --> D["Final Check<br/>Quality Gate & Validation"]
    D --> E["MOVIES_MASTER_FINAL.csv<br/>Production Dataset"]
    
    style E fill:#90EE90
    style A fill:#FFE4B5
```

### ğŸ¥ **Active Modules:**

1. **ğŸ¬ movie_estandarizer** - Transaction standardization (17Mâ†’2K records)
2. **ğŸŒ MOVIES_INFO** - Web scraping utilities for metadata
3. **ğŸš€ parche_output** - Advanced enrichment via multi-source scraping
4. **ğŸ¯ final_check** - **PRODUCTION MODULE** - Quality certification (2,372Ã—16 dataset)

### ğŸ“Š **Additional Modules:**
5. **ğŸ’³ order_range_recognition** - Payment analysis (standalone utility)

---

## ğŸš€ **Quick Start**

### **Acceso a Archivos Finales**
```bash
# Clonar el repositorio
git clone <repository-url>
cd help_frida

# ğŸ¯ ARCHIVO PRINCIPAL DE PRODUCCIÃ“N
ls -la final_check/MOVIES_MASTER_FINAL.csv
# Output: Dataset completo (2372Ã—16, 1MB)

# ğŸ“ ARCHIVO DE ANÃLISIS DE DESCRIPCIONES
ls -la final_check/descripcion.csv  
# Output: Mapeo Ãºnico (885Ã—2, 133KB)

# Validar ambos archivos
cd final_check
python validate_dataset.py
```

### **Uso de los Archivos Finales**
```python
import pandas as pd

# ğŸ¯ Cargar dataset principal completo
master_df = pd.read_csv('final_check/MOVIES_MASTER_FINAL.csv')
print(f"Dataset principal: {len(master_df)} registros, {len(master_df.columns)} columnas")

# ğŸ“ Cargar mapeo de descripciones Ãºnicas
desc_df = pd.read_csv('final_check/descripcion.csv')
print(f"Descripciones Ãºnicas: {len(desc_df)} pelÃ­culas")

# AnÃ¡lisis especializado por contenido
terror_movies = desc_df[desc_df['DESCRIPCION'].str.contains('terror|horror', case=False)]
print(f"PelÃ­culas de terror: {len(terror_movies)}")
```

### **Regenerar Archivos (si necesario)**
```bash
cd final_check

# Regenerar dataset principal
python final_processor.py
# Output: MOVIES_MASTER_FINAL.csv (2372Ã—16, 100% calidad)

# Regenerar mapeo de descripciones
python generate_descripcion.py  
# Output: descripcion.csv (885Ã—2, mapeo Ãºnico)
```

### **Data Source Requirements**
The pipeline processes data from this SQL query:
```sql
SELECT * FROM DATA_SHARE_ATHENA.PUBLIC.ATHENA_CINEPOLIS_MOVIE_DETAIL
```

### **System Requirements**
```bash
# Install dependencies
pip install pandas numpy requests beautifulsoup4 tqdm
```

## ğŸ—‚ï¸ Repository Structure

```
help_frida/
â”œâ”€â”€ ğŸ“ movie_estandarizer/           # ğŸ¬ Transaction standardization (17Mâ†’2K)
â”‚   â”œâ”€â”€ estandarizer.py             # Main standardization engine
â”‚   â”œâ”€â”€ movie_enricher.py           # Web scraping enrichment
â”‚   â”œâ”€â”€ complete_pipeline.py        # Full processing pipeline
â”‚   â””â”€â”€ input_data/
â”‚       â”œâ”€â”€ Cinepolis.csv           # Source data (5GB)
â”‚       â””â”€â”€ Cinepolis_sample.csv    # Sample for testing
â”œâ”€â”€ ğŸ“ parche_output/                # ğŸš€ Advanced enrichment system
â”‚   â”œâ”€â”€ GOAT_enrichment.py          # Multi-source web scraping
â”‚   â”œâ”€â”€ data_quality_evaluator.py  # Quality analysis
â”‚   â””â”€â”€ *.json                      # Metrics and cache files
â”œâ”€â”€ ğŸ“ final_check/ â­              # ğŸ¯ PRODUCTION MODULE
â”‚   â”œâ”€â”€ MOVIES_MASTER_FINAL.csv     # ğŸ¬ FINAL DATASET (2372Ã—16)
â”‚   â”œâ”€â”€ final_processor.py          # Main processing script v2.0
â”‚   â”œâ”€â”€ validate_dataset.py         # Quick validation tool
â”‚   â””â”€â”€ README.md                   # Detailed module documentation
â”œâ”€â”€ ğŸ“ MOVIES_INFO/                  # ğŸŒ Web scraping utilities
â”œâ”€â”€ ğŸ“ order_range_recognition/      # ğŸ’³ Payment analysis tools
â””â”€â”€ ğŸ“„ README.md                     # ğŸ“š This documentation
```

> **Note**: Only essential files are tracked in Git. Large data files (>100MB) are excluded via .gitignore.

---

## ğŸ“‹ **Module Documentation**

### ğŸ¬ Module 1: Movie Standardizer (`movie_estandarizer/`)

### Purpose
**This framework transforms 17+ million transactional cinema records into a deduplicated, standardized movie catalog, enabling robust analytics and business intelligence on unique film titles rather than individual transactions.**

The system automatically identifies and consolidates **multiple movie name variations** into canonical representations, solving the critical challenge of:

- **Name Variation Management**: Consolidates "Superman Esp", "Superman ESP", "Superman 4DX/3D" into a single entity
- **Format-agnostic Deduplication**: Groups all versions (2D, 3D, 4DX, IMAX) of the same movie
- **Language Variant Unification**: Merges ESP, SUB, and DUB versions under unified movie families
- **Enrichment Pipeline**: Provides structure for metadata augmentation from external sources
- **Scalable Processing**: Handles 5GB+ input files with streaming architecture

### Directory Structure
```
movie_estandarizer/
â”œâ”€â”€ input_data/
â”‚   â”œâ”€â”€ Cinepolis.csv          # âš ï¸ Input data (execute SQL query first)
â”‚   â””â”€â”€ input_query.sql        # SQL query to obtain data
â”œâ”€â”€ output_data/
â”‚   â”œâ”€â”€ example_output_data.csv    # Template file (auto-created if missing)
â”‚   â”œâ”€â”€ output_data.csv            # Unique movies catalog
â”‚   â”œâ”€â”€ output_data_all.csv        # All records with IDs
â”‚   â””â”€â”€ output_data_final.csv      # Enriched final output
â”œâ”€â”€ estandarizer.py            # Core standardization engine
â”œâ”€â”€ movie_enricher.py          # Web enrichment module
â”œâ”€â”€ complete_pipeline.py       # Full automated pipeline
â”œâ”€â”€ requirements.txt           # Python dependencies
â””â”€â”€ README.md                  # Detailed module documentation
```

### ğŸ—ï¸ Core Transformation Principle

**The fundamental concept is multi-level title normalization with intelligent deduplication:**

#### Hierarchical Normalization Strategy

```
Original Transaction â†’ Movie Name â†’ Clean Title â†’ Family Group â†’ Unique Identifier
"Superman 4DX/3D Esp" â†’ "SUPERMAN 4DX/3D ESP" â†’ "SUPERMAN" â†’ "SUPERMAN" â†’ ID:123
"Superman Esp"        â†’ "SUPERMAN ESP"        â†’ "SUPERMAN" â†’ "SUPERMAN" â†’ ID:123
"Superman IMAX Sub"   â†’ "SUPERMAN IMAX SUB"   â†’ "SUPERMAN" â†’ "SUPERMAN" â†’ ID:123
```

This multi-tier approach ensures **robust deduplication** while preserving format and language information for business analytics.

### ğŸ“Š Processing Pipeline

```mermaid
graph TD
    A["Input: Cinepolis.csv\n17M+ Records"] --> B["Standardizer\nestandarizer.py"]
    B --> C["Unique Movie Catalog\n~2K Movies"]
    B --> D["Full Record Mapping\n17M Records with IDs"]
    D --> E["Data Propagation\nmovie_enricher.py"]
    E --> F["Web Enrichment\nCinÃ©polis Scraping"]
    F --> G["Final Consistency Check"]
    G --> H["Complete Dataset\noutput_data_final.csv"]
```

### ğŸ”„ Three Processing Modes

#### 1ï¸âƒ£ Complete Pipeline (Recommended)
```bash
cd movie_estandarizer
python complete_pipeline.py
```
- Processes ALL 17M+ records
- Maps each to unique movie IDs
- Enriches data from web sources
- Ensures final consistency
- Output: `output_data_final.csv`

#### 2ï¸âƒ£ Catalog Mode (Unique Movies Only)
```bash
python estandarizer.py
```
- Extracts ~2,000 unique movies
- Creates deduplicated catalog
- Output: `output_data.csv`

#### 3ï¸âƒ£ Full Mapping Mode (All Records)
```bash
python estandarizer.py --all-records
```
- Processes ALL records
- Maintains transaction-level detail
- Maps to unique movie IDs
- Output: `output_data_all.csv`

### ğŸš€ Quick Start Guide

#### Step 1: Prepare Input Data
```bash
# Execute SQL query and save results
1. Run query from: input_data/input_query.sql
2. Export to CSV: input_data/Cinepolis.csv
```

#### Step 2: Run Processing
```bash
# Complete pipeline (recommended)
cd movie_estandarizer
python complete_pipeline.py
```

### ğŸ’» Core Components

1. **MovieStandardizer Class** (`estandarizer.py`)
   - Multi-level normalization algorithms
   - Format and language extraction
   - Family detection
   - Movie ID assignment
   - Batch processing of large files

2. **MovieEnricher Class** (`movie_enricher.py`)
   - Data propagation between related records
   - Google search for missing information
   - Web scraping from CinÃ©polis website
   - Structured data extraction
   - Final consistency validation

3. **Complete Pipeline** (`complete_pipeline.py`)
   - Orchestrates the entire process
   - Integrates standardization and enrichment
   - Error handling and recovery
   - Performance optimization
   - Comprehensive logging

### ğŸ”§ Advanced Features

- **Missing File Handling**: Auto-creates empty templates
- **Input Validation**: Checks for required columns
- **Consistency Enforcement**: Ensures data integrity
- **Batch Optimization**: Processes 500K records at a time
- **Robust Error Recovery**: Continues on partial failures

### âœ… Output Files
- `output_data_all.csv`: All 17M records with movie IDs
- `output_data_final.csv`: Enriched complete dataset
- `output_data.csv`: Unique movies catalog only

> ğŸ“ **Sample Output Data**: A sample version of the processed movie data (`output_data.csv`) is available on [Google Drive](https://drive.google.com/drive/folders/1FzNKnWKWpBXWzYgMD24FJCjkY8wIcwtW?usp=drive_link) for reference and testing.

---

## ğŸ’³ Module 2: Order Range Recognition (`order_range_recognition/`)

### Purpose
Identifies optimal payment amount ranges using Bayesian histogram optimization for statistical analysis and business intelligence.

### Directory Structure
```
order_range_recognition/
â”œâ”€â”€ query_output/
â”‚   â””â”€â”€ ECR.csv                # Query results (if any)
â”œâ”€â”€ ia_AR_order_amount_range_of_interest.sql   # Acceptance Rate analysis
â”œâ”€â”€ ia_ECR_order_amount_range_of_interest.sql  # Effective Cost Rate analysis
â””â”€â”€ README.md                  # Module documentation
```

### ğŸš€ Usage

#### Step 1: Execute SQL Queries

**For Acceptance Rate Analysis**:
```sql
-- Execute: order_range_recognition/ia_AR_order_amount_range_of_interest.sql
-- Export results to: order_range_recognition/query_output/AR.csv
```

**For Effective Cost Rate Analysis**:
```sql
-- Execute: order_range_recognition/ia_ECR_order_amount_range_of_interest.sql  
-- Export results to: order_range_recognition/query_output/ECR.csv
-- (REPLACE the existing sample ECR.csv file)
```

#### Step 2: Customize Parameters
- Replace `COMMERCE_ID` parameter in both SQL files with your target merchant ID
- Current default: `'9ea20bdb-5cff-4b10-9c95-9cebf8b6ddb4'`

### Key Features
- Bayesian optimal binning
- Logarithmic transformation for skewed data
- Automatic gap detection
- Strategic interval categorization (LARGE/MEDIUM/SMALL)

### Output
- Contiguous payment ranges without gaps
- Statistically optimal intervals
- Ready for A/B testing and sampling

---

## ğŸŒ Module 3: Movies Info (`MOVIES_INFO/`)

### Purpose
Web scraping utilities for extracting movie metadata from CinÃ©polis Chile website.

### Directory Structure
```
MOVIES_INFO/
â”œâ”€â”€ movies/                    # Scraped movie data
â”‚   â”œâ”€â”€ 200-LOBO/
â”‚   â”‚   â”œâ”€â”€ description.txt
â”‚   â”‚   â”œâ”€â”€ texto_estructurado.txt
â”‚   â”‚   â””â”€â”€ webpage.txt
â”‚   â””â”€â”€ [other movies...]
â”œâ”€â”€ wget.py                    # Main scraping script
â”œâ”€â”€ main_page.html            # Cached main page
â””â”€â”€ README.MD                 # Module documentation
```

### ğŸš€ Usage

```bash
cd MOVIES_INFO
python wget.py

# The script will:
# 1. Download CinÃ©polis main page
# 2. Extract movie URLs
# 3. Scrape each movie's metadata
# 4. Save structured data in movies/
```

### Extracted Data
For each movie:
- `description.txt`: Full page text
- `texto_estructurado.txt`: Structured metadata
  - Movie name
  - Age restriction
  - Duration
  - Category
  - Synopsis
  - Actors
  - Directors
- `webpage.txt`: Source URL

### Key Features
- Automatic movie discovery
- Structured data extraction
- Rate limiting to avoid server overload
- Error handling and recovery

---

## ğŸš€ Module 4: Parche Output (`parche_output/`)

### Purpose
**Final enrichment patch that completes movie metadata by performing exhaustive web searches across multiple sources (CinÃ©polis, Wikipedia, IMDb) and applying intelligent data replication strategies to minimize empty cells.**

This module takes the output from `movie_estandarizer` and enhances it to achieve maximum data completeness - reducing empty cells from 35% to less than 9%.

### Directory Structure
```
parche_output/
â”œâ”€â”€ GOAT_enrichment.py         # Main enrichment engine
â”œâ”€â”€ README.MD                  # Detailed documentation
â”œâ”€â”€ output_data.csv           # âš ï¸ Input (copy from movie_estandarizer)
â”œâ”€â”€ standardized.csv          # Final enriched output (33GB+)
â”œâ”€â”€ movie_cache.json          # Web search cache
â””â”€â”€ enrichment_metrics.json   # Performance metrics
```

### ğŸ—ï¸ Core Transformation Principle

**Multi-source web enrichment with dual replication strategies:**

1. **Web Scraping**: Searches CinÃ©polis, Wikipedia, IMDb for missing metadata
2. **Vertical Replication**: Propagates data across rows with same movie name
3. **Horizontal Replication**: Duplicates data across related columns

### ğŸš€ Usage

#### Step 1: Prepare Input Data
```bash
# First generate output_data.csv with movie_estandarizer
cd movie_estandarizer
python complete_pipeline.py

# Copy the output to parche_output
cd ../parche_output
cp ../movie_estandarizer/output_data/output_data.csv ./
```

#### Step 2: Run Enrichment
```bash
cd parche_output
python GOAT_enrichment.py

# Processing time: ~50 minutes for 868 unique movies
# Output: standardized.csv (33GB with 7.5M rows)
```

### ğŸ“Š Results
- **Empty cells**: 35.29% â†’ 8.63% (76% reduction)
- **Descriptions**: 100% complete
- **Categories**: 90.3% filled
- **Actors**: 67.5% filled
- **Directors**: 5.3% filled

### âš ï¸ Important Notes
- Input file `output_data.csv` must be copied from movie_estandarizer
- Output file `standardized.csv` will be ~33GB
- Both CSV files are excluded from Git via .gitignore
- Uses caching to avoid duplicate web searches

---

## âœ… Module 5: Final Check (`final_check/`) - ğŸ¯ **PRODUCTION GATEWAY**

### Purpose
**Critical quality gate that validates, enriches, and certifies the definitive Movies Master dataset with 100.0/100 quality score for `EXTERNAL_SOURCES.CINEPOLIS.MOVIES_MASTER`.**

This module ensures production-ready data with perfect coherence, optimal completeness (98.7%), and professional formatting across all 2,372 records.

### ğŸ“Š **Final Dataset Metrics**
- **Structure**: 2,372 rows Ã— 16 columns
- **Unique Movies**: 885 films
- **Completeness**: 98.7% (37,457/37,952 cells)
- **Quality Score**: 100.0/100 âœ…
- **Categories**: 69 unique single-word labels
- **Perfect Mappings**: 1:1 NOMBRE_UNICO â†” MOVIE_ID

### Directory Structure
```
final_check/
â”œâ”€â”€ ğŸ“„ MOVIES_MASTER_FINAL.csv      # ğŸ¯ PRODUCTION DATASET (2372Ã—16, ~1MB)
â”œâ”€â”€ ğŸ“Š FINAL_QUALITY_METRICS.json   # Detailed quality metrics
â”œâ”€â”€ ğŸ“Š FINAL_STATISTICS_REPORT.json # Comprehensive statistics
â”œâ”€â”€ ğŸ”§ final_processor.py           # Main processing script v2.0
â”œâ”€â”€ ğŸ“– README.md                     # Module documentation
â”œâ”€â”€ ğŸ“‹ requirements.txt              # Dependencies (pandas, numpy)
â””â”€â”€ ğŸ“„ info_descargada_a_mano.txt   # Reference data
```

### ğŸ—ï¸ Core Transformation Pipeline

**Six-phase approach for production perfection:**

1. **Advanced Cleaning**: Symbol removal, normalization
2. **Horizontal Coherence**: IDIOMA â†” MOVIE_NAME validation
3. **Vertical Coherence**: Consistent metadata per movie
4. **Perfect Remapping**: 1:1 MOVIE_ID assignment
5. **Professional Format**: Capitalization, standardization
6. **Final Validation**: 8 critical quality checks

### ğŸ“Š Processing Results

**Completitud por Campo:**
- âœ… **100% Complete** (13/16 fields): MOVIE_ID, MOVIE_NAME, TITULO_LIMPIO, FORMATO, IDIOMA, CATEGORIA, FAMILIA, NOMBRE_ORIGINAL, CATEGORIA_CINEPOLIS, NOMBRE_ORIGINAL_CLEAN, TITULO_LIMPIO_CLEAN, NOMBRE_UNICO, DESCRIPCION
- âš ï¸ **High Completeness**: DURACION (99.7%), DIRECTOR (91.8%), ACTOR_PRINCIPAL (87.6%)

### ğŸš€ **Usage**

#### ğŸ¯ **Quick Validation**
```bash
cd final_check

# Procesar y certificar dataset principal
python final_processor.py
# âœ… Genera: MOVIES_MASTER_FINAL.csv (2372Ã—16, 1MB)
# âœ… Genera: FINAL_QUALITY_METRICS.json
# âœ… Genera: FINAL_STATISTICS_REPORT.json

# Generar mapeo de descripciones especializadas
python generate_descripcion.py
# âœ… Genera: descripcion.csv (885Ã—2, 133KB)
# âœ… ValidaciÃ³n: Sin duplicados, 100% completitud

# ValidaciÃ³n rÃ¡pida de ambos archivos
python validate_dataset.py
# Expected: "Dataset certified for production" âœ…
```

### ğŸ† **Production Certification**
```
ğŸ† QUALITY SCORE: 100.0/100
âœ… Structure: 2372Ã—16
âœ… Completeness: 98.7%
âœ… Coherence: Perfect
âœ… Validation: 10/10 checks passed
âœ… Ready for: EXTERNAL_SOURCES.CINEPOLIS.MOVIES_MASTER
```

## ğŸ¯ **DESTINO FINAL DE PRODUCCIÃ“N**

### ğŸ“¦ **Archivos Listos para Deploy**

#### ğŸ¬ **Dataset Principal**
```sql
TARGET TABLE: EXTERNAL_SOURCES.CINEPOLIS.MOVIES_MASTER
SOURCE FILE: final_check/MOVIES_MASTER_FINAL.csv
RECORDS: 2,372 Ã— 16 columnas
QUALITY: 100.0/100
STATUS: âœ… CERTIFICADO PARA PRODUCCIÃ“N
```

#### ğŸ“ **Dataset de AnÃ¡lisis**  
```
PURPOSE: Text Mining & Content Analysis
SOURCE FILE: final_check/descripcion.csv
RECORDS: 885 pelÃ­culas Ãºnicas Ã— 2 columnas (NOMBRE_UNICO, DESCRIPCION)
USE CASES: BÃºsquedas temÃ¡ticas, anÃ¡lisis de contenido, clasificaciÃ³n automÃ¡tica
STATUS: âœ… LISTO PARA ANÃLISIS
```

**El pipeline completo culmina en DOS archivos de producciÃ³n:**
1. **`final_check/MOVIES_MASTER_FINAL.csv`** - Dataset completo validado al 100% para el catÃ¡logo maestro
2. **`final_check/descripcion.csv`** - Mapeo especializado para anÃ¡lisis avanzado de descripciones

---

## ğŸ”§ Installation & Setup

### Prerequisites
```bash
# Python 3.8+
python --version

# Install dependencies for each module
cd movie_estandarizer
pip install -r requirements.txt
```

### Required Python Packages
```python
pandas>=2.0.0
numpy>=1.24.0
requests>=2.28.0
beautifulsoup4>=4.11.0
tqdm>=4.65.0
```

---

## ğŸ“Š Data Flow - Pipeline Completo

```mermaid
graph LR
    A[Raw Transactions<br/>17M+ records] --> B[movie_estandarizer<br/>Deduplication]
    B --> C[parche_output<br/>Web Enrichment]
    C --> D[final_check<br/>Quality Gate]
    D --> E[MOVIES_MASTER_FINAL.csv<br/>2372Ã—16]
    D --> F[descripcion.csv<br/>885Ã—2]
    E --> G[EXTERNAL_SOURCES.CINEPOLIS.MOVIES_MASTER]
    F --> H[Text Analysis & Mining]
    
    style E fill:#4caf50,color:#fff
    style F fill:#2196f3,color:#fff
    style G fill:#ff5722,color:#fff
    style H fill:#9c27b0,color:#fff
```

---

## ğŸš¦ Processing Pipeline

### Complete Workflow

1. **Data Extraction**
   ```sql
   -- Run SQL queries to get raw data
   -- Export to CSV format
   ```

2. **Standardization**
   ```bash
   cd movie_estandarizer
   python estandarizer.py --all-records
   ```

3. **Enrichment**
   ```bash
   cd parche_output
   python GOAT_enrichment.py
   ```

4. **Final Check & Production**
   ```bash
   cd final_check
   
   # Procesar y certificar dataset principal
   python final_processor.py
   # âœ… Output: MOVIES_MASTER_FINAL.csv (2372Ã—16, 100% calidad)
   
   # Generar mapeo de descripciones
   python generate_descripcion.py
   # âœ… Output: descripcion.csv (885Ã—2, sin duplicados)
   
   # Validar ambos archivos
   python validate_dataset.py
   # Expected: "Dataset certified for production" âœ…
   ```

5. **Deploy to Production**
   ```sql
   -- Dataset Principal
   LOAD DATA FROM 'final_check/MOVIES_MASTER_FINAL.csv'
   INTO TABLE EXTERNAL_SOURCES.CINEPOLIS.MOVIES_MASTER;
   
   -- AnÃ¡lisis Especializado
   USE 'final_check/descripcion.csv' FOR text_analysis, content_mining;
   ```

---

## ğŸ“ˆ Performance Metrics

### Movie Standardizer
- **Input**: 17M+ records (5GB)
- **Output**: ~2,000 unique movies
- **Processing**: ~200,000 records/second
- **Memory**: 4GB RAM max
- **Time**: 15-20 minutes total

### Web Scraping
- **Speed**: ~50 movies/minute
- **Success Rate**: ~80% extraction
- **Retry Logic**: Automatic on failures

---

## ğŸ” Key Algorithms

### Movie Deduplication
```python
# Multi-level normalization
Raw Name â†’ TITULO_LIMPIO â†’ NOMBRE_UNICO â†’ FAMILIA
"Superman 4DX Esp" â†’ "SUPERMAN 4DX ESP" â†’ "SUPERMAN" â†’ "SUPERMAN"
```

### Bayesian Binning
```sql
-- Knuth's Bayesian optimization
-- Finds optimal bin count M*
-- Maximizes marginal likelihood
-- No arbitrary parameters needed
```

---

## ğŸ›¡ï¸ Error Handling

### Missing Input Files
```bash
âŒ ERROR: Input file not found!
Please follow these steps:
1. Execute the SQL query
2. Export results to CSV
3. Save to appropriate location
```

### Automatic Recovery
- Creates missing directories
- Generates empty templates
- Continues on partial failures
- Logs all errors for review

---

## ğŸ“ Logging

All modules generate detailed logs:
- `movie_standardizer.log`
- `movie_enricher.log`
- `complete_pipeline.log`