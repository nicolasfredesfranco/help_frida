# Help Frida - Multi-Module Data Processing Repository

![Movie Processing](https://img.shields.io/badge/Movies-Standardization-blue)
![Payment Analysis](https://img.shields.io/badge/Payments-Bayesian_Analysis-purple)
![Web Scraping](https://img.shields.io/badge/Enrichment-Web_Scraping-green)
![Status](https://img.shields.io/badge/Status-Production-brightgreen)

## üìã Repository Overview

This repository contains four specialized data processing modules for different analytical purposes:

1. **Movie Standardizer** - Cinema transaction deduplication and enrichment
2. **Order Range Recognition** - Bayesian payment amount analysis
3. **Movies Info** - Web scraping utilities for movie metadata
4. **Parche Output** - Final enrichment patch with comprehensive web scraping

---

## **üìä Data Generation Process**

**IMPORTANT**: All CSV data files are generated by executing SQL queries located in each module folder.

### **Current Workflow:**
1. **Execute SQL Query**: Run queries from respective `input_query.sql` files:
   - `movie_estandarizer/input_data/input_query.sql`
   - `order_range_recognition/ia_AR_order_amount_range_of_interest.sql`
   - `order_range_recognition/ia_ECR_order_amount_range_of_interest.sql`

2. **Download Data**: Export query results as CSV format

3. **Local Processing**: Place CSV files in appropriate directories and run Python scripts

### **Future Roadmap:**
The codebase is designed for **Snowflake Notebook integration** using Snowflake connectors:
- **Direct SQL-to-DataFrame** processing without intermediate CSV files
- **Automated execution** within Snowflake containers or notebooks
- **Monthly automated processing** through scheduled jobs
- **Elimination of manual** CSV download/upload steps

**The current local implementation serves as a foundation for seamless migration to cloud-native Snowflake execution.**

## üóÇÔ∏è Repository Structure

```
help_frida/
‚îú‚îÄ‚îÄ movie_estandarizer/        # üé¨ Movie standardization and enrichment
‚îú‚îÄ‚îÄ order_range_recognition/   # üí≥ Payment amount range analysis
‚îú‚îÄ‚îÄ MOVIES_INFO/               # üåê Movie metadata scraping
‚îú‚îÄ‚îÄ parche_output/             # üöÄ Final enrichment patch with web scraping
‚îî‚îÄ‚îÄ README.md                  # üìö This documentation
```

---

## üé¨ Module 1: Movie Standardizer (`movie_estandarizer/`)

### Purpose
**This framework transforms 17+ million transactional cinema records into a deduplicated, standardized movie catalog, enabling robust analytics and business intelligence on unique film titles rather than individual transactions.**

The system automatically identifies and consolidates **multiple movie name variations** into canonical representations, solving the critical challenge of:

- **Name Variation Management**: Consolidates "Superman Esp", "Superman ESP", "Superman 4DX/3D" into a single entity
- **Format-agnostic Deduplication**: Groups all versions (2D, 3D, 4DX, IMAX) of the same movie
- **Language Variant Unification**: Merges ESP, SUB, and DUB versions under unified movie families
- **Enrichment Pipeline**: Provides structure for metadata augmentation from external sources
- **Scalable Processing**: Handles 5GB+ input files with streaming architecture

### Directory Structure
```
movie_estandarizer/
‚îú‚îÄ‚îÄ input_data/
‚îÇ   ‚îú‚îÄ‚îÄ Cinepolis.csv          # ‚ö†Ô∏è Input data (execute SQL query first)
‚îÇ   ‚îî‚îÄ‚îÄ input_query.sql        # SQL query to obtain data
‚îú‚îÄ‚îÄ output_data/
‚îÇ   ‚îú‚îÄ‚îÄ example_output_data.csv    # Template file (auto-created if missing)
‚îÇ   ‚îú‚îÄ‚îÄ output_data.csv            # Unique movies catalog
‚îÇ   ‚îú‚îÄ‚îÄ output_data_all.csv        # All records with IDs
‚îÇ   ‚îî‚îÄ‚îÄ output_data_final.csv      # Enriched final output
‚îú‚îÄ‚îÄ estandarizer.py            # Core standardization engine
‚îú‚îÄ‚îÄ movie_enricher.py          # Web enrichment module
‚îú‚îÄ‚îÄ complete_pipeline.py       # Full automated pipeline
‚îú‚îÄ‚îÄ requirements.txt           # Python dependencies
‚îî‚îÄ‚îÄ README.md                  # Detailed module documentation
```

### üèóÔ∏è Core Transformation Principle

**The fundamental concept is multi-level title normalization with intelligent deduplication:**

#### Hierarchical Normalization Strategy

```
Original Transaction ‚Üí Movie Name ‚Üí Clean Title ‚Üí Family Group ‚Üí Unique Identifier
"Superman 4DX/3D Esp" ‚Üí "SUPERMAN 4DX/3D ESP" ‚Üí "SUPERMAN" ‚Üí "SUPERMAN" ‚Üí ID:123
"Superman Esp"        ‚Üí "SUPERMAN ESP"        ‚Üí "SUPERMAN" ‚Üí "SUPERMAN" ‚Üí ID:123
"Superman IMAX Sub"   ‚Üí "SUPERMAN IMAX SUB"   ‚Üí "SUPERMAN" ‚Üí "SUPERMAN" ‚Üí ID:123
```

This multi-tier approach ensures **robust deduplication** while preserving format and language information for business analytics.

### üìä Processing Pipeline

```mermaid
graph TD
    A["Input: Cinepolis.csv\n17M+ Records"] --> B["Standardizer\nestandarizer.py"]
    B --> C["Unique Movie Catalog\n~2K Movies"]
    B --> D["Full Record Mapping\n17M Records with IDs"]
    D --> E["Data Propagation\nmovie_enricher.py"]
    E --> F["Web Enrichment\nCin√©polis Scraping"]
    F --> G["Final Consistency Check"]
    G --> H["Complete Dataset\noutput_data_final.csv"]
```

### üîÑ Three Processing Modes

#### 1Ô∏è‚É£ Complete Pipeline (Recommended)
```bash
cd movie_estandarizer
python complete_pipeline.py
```
- Processes ALL 17M+ records
- Maps each to unique movie IDs
- Enriches data from web sources
- Ensures final consistency
- Output: `output_data_final.csv`

#### 2Ô∏è‚É£ Catalog Mode (Unique Movies Only)
```bash
python estandarizer.py
```
- Extracts ~2,000 unique movies
- Creates deduplicated catalog
- Output: `output_data.csv`

#### 3Ô∏è‚É£ Full Mapping Mode (All Records)
```bash
python estandarizer.py --all-records
```
- Processes ALL records
- Maintains transaction-level detail
- Maps to unique movie IDs
- Output: `output_data_all.csv`

### üöÄ Quick Start Guide

#### Step 1: Prepare Input Data
```bash
# Execute SQL query and save results
1. Run query from: input_data/input_query.sql
2. Export to CSV: input_data/Cinepolis.csv
```

#### Step 2: Run Processing
```bash
# Complete pipeline (recommended)
cd movie_estandarizer
python complete_pipeline.py
```

### üíª Core Components

1. **MovieStandardizer Class** (`estandarizer.py`)
   - Multi-level normalization algorithms
   - Format and language extraction
   - Family detection
   - Movie ID assignment
   - Batch processing of large files

2. **MovieEnricher Class** (`movie_enricher.py`)
   - Data propagation between related records
   - Google search for missing information
   - Web scraping from Cin√©polis website
   - Structured data extraction
   - Final consistency validation

3. **Complete Pipeline** (`complete_pipeline.py`)
   - Orchestrates the entire process
   - Integrates standardization and enrichment
   - Error handling and recovery
   - Performance optimization
   - Comprehensive logging

### üîß Advanced Features

- **Missing File Handling**: Auto-creates empty templates
- **Input Validation**: Checks for required columns
- **Consistency Enforcement**: Ensures data integrity
- **Batch Optimization**: Processes 500K records at a time
- **Robust Error Recovery**: Continues on partial failures

### ‚úÖ Output Files
- `output_data_all.csv`: All 17M records with movie IDs
- `output_data_final.csv`: Enriched complete dataset
- `output_data.csv`: Unique movies catalog only

> üìÅ **Sample Output Data**: A sample version of the processed movie data (`output_data.csv`) is available on [Google Drive](https://drive.google.com/drive/folders/1FzNKnWKWpBXWzYgMD24FJCjkY8wIcwtW?usp=drive_link) for reference and testing.

---

## üí≥ Module 2: Order Range Recognition (`order_range_recognition/`)

### Purpose
Identifies optimal payment amount ranges using Bayesian histogram optimization for statistical analysis and business intelligence.

### Directory Structure
```
order_range_recognition/
‚îú‚îÄ‚îÄ query_output/
‚îÇ   ‚îî‚îÄ‚îÄ ECR.csv                # Query results (if any)
‚îú‚îÄ‚îÄ ia_AR_order_amount_range_of_interest.sql   # Acceptance Rate analysis
‚îú‚îÄ‚îÄ ia_ECR_order_amount_range_of_interest.sql  # Effective Cost Rate analysis
‚îî‚îÄ‚îÄ README.md                  # Module documentation
```

### üöÄ Usage

#### Step 1: Execute SQL Queries

**For Acceptance Rate Analysis**:
```sql
-- Execute: order_range_recognition/ia_AR_order_amount_range_of_interest.sql
-- Export results to: order_range_recognition/query_output/AR.csv
```

**For Effective Cost Rate Analysis**:
```sql
-- Execute: order_range_recognition/ia_ECR_order_amount_range_of_interest.sql  
-- Export results to: order_range_recognition/query_output/ECR.csv
-- (REPLACE the existing sample ECR.csv file)
```

#### Step 2: Customize Parameters
- Replace `COMMERCE_ID` parameter in both SQL files with your target merchant ID
- Current default: `'9ea20bdb-5cff-4b10-9c95-9cebf8b6ddb4'`

### Key Features
- Bayesian optimal binning
- Logarithmic transformation for skewed data
- Automatic gap detection
- Strategic interval categorization (LARGE/MEDIUM/SMALL)

### Output
- Contiguous payment ranges without gaps
- Statistically optimal intervals
- Ready for A/B testing and sampling

---

## üåê Module 3: Movies Info (`MOVIES_INFO/`)

### Purpose
Web scraping utilities for extracting movie metadata from Cin√©polis Chile website.

### Directory Structure
```
MOVIES_INFO/
‚îú‚îÄ‚îÄ movies/                    # Scraped movie data
‚îÇ   ‚îú‚îÄ‚îÄ 200-LOBO/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ description.txt
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ texto_estructurado.txt
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ webpage.txt
‚îÇ   ‚îî‚îÄ‚îÄ [other movies...]
‚îú‚îÄ‚îÄ wget.py                    # Main scraping script
‚îú‚îÄ‚îÄ main_page.html            # Cached main page
‚îî‚îÄ‚îÄ README.MD                 # Module documentation
```

### üöÄ Usage

```bash
cd MOVIES_INFO
python wget.py

# The script will:
# 1. Download Cin√©polis main page
# 2. Extract movie URLs
# 3. Scrape each movie's metadata
# 4. Save structured data in movies/
```

### Extracted Data
For each movie:
- `description.txt`: Full page text
- `texto_estructurado.txt`: Structured metadata
  - Movie name
  - Age restriction
  - Duration
  - Category
  - Synopsis
  - Actors
  - Directors
- `webpage.txt`: Source URL

### Key Features
- Automatic movie discovery
- Structured data extraction
- Rate limiting to avoid server overload
- Error handling and recovery

---

## üöÄ Module 4: Parche Output (`parche_output/`)

### Purpose
**Final enrichment patch that completes movie metadata by performing exhaustive web searches across multiple sources (Cin√©polis, Wikipedia, IMDb) and applying intelligent data replication strategies to minimize empty cells.**

This module takes the output from `movie_estandarizer` and enhances it to achieve maximum data completeness - reducing empty cells from 35% to less than 9%.

### Directory Structure
```
parche_output/
‚îú‚îÄ‚îÄ GOAT_enrichment.py         # Main enrichment engine
‚îú‚îÄ‚îÄ README.MD                  # Detailed documentation
‚îú‚îÄ‚îÄ output_data.csv           # ‚ö†Ô∏è Input (copy from movie_estandarizer)
‚îú‚îÄ‚îÄ standardized.csv          # Final enriched output (33GB+)
‚îú‚îÄ‚îÄ movie_cache.json          # Web search cache
‚îî‚îÄ‚îÄ enrichment_metrics.json   # Performance metrics
```

### üèóÔ∏è Core Transformation Principle

**Multi-source web enrichment with dual replication strategies:**

1. **Web Scraping**: Searches Cin√©polis, Wikipedia, IMDb for missing metadata
2. **Vertical Replication**: Propagates data across rows with same movie name
3. **Horizontal Replication**: Duplicates data across related columns

### üöÄ Usage

#### Step 1: Prepare Input Data
```bash
# First generate output_data.csv with movie_estandarizer
cd movie_estandarizer
python complete_pipeline.py

# Copy the output to parche_output
cd ../parche_output
cp ../movie_estandarizer/output_data/output_data.csv ./
```

#### Step 2: Run Enrichment
```bash
cd parche_output
python GOAT_enrichment.py

# Processing time: ~50 minutes for 868 unique movies
# Output: standardized.csv (33GB with 7.5M rows)
```

### üìä Results
- **Empty cells**: 35.29% ‚Üí 8.63% (76% reduction)
- **Descriptions**: 100% complete
- **Categories**: 90.3% filled
- **Actors**: 67.5% filled
- **Directors**: 5.3% filled

### ‚ö†Ô∏è Important Notes
- Input file `output_data.csv` must be copied from movie_estandarizer
- Output file `standardized.csv` will be ~33GB
- Both CSV files are excluded from Git via .gitignore
- Uses caching to avoid duplicate web searches

---

## üîß Installation & Setup

### Prerequisites
```bash
# Python 3.8+
python --version

# Install dependencies for each module
cd movie_estandarizer
pip install -r requirements.txt
```

### Required Python Packages
```python
pandas>=2.0.0
numpy>=1.24.0
requests>=2.28.0
beautifulsoup4>=4.11.0
tqdm>=4.65.0
```

---

## üìä Data Flow

```mermaid
graph LR
    A[SQL Database] -->|Query| B[CSV Files]
    B --> C[movie_estandarizer]
    C --> D[Standardized Data]
    D --> E[movie_enricher]
    F[Cin√©polis Web] -->|Scraping| E
    E --> G[Final Enriched Dataset]
```

---

## üö¶ Processing Pipeline

### Complete Workflow

1. **Data Extraction**
   ```sql
   -- Run SQL queries to get raw data
   -- Export to CSV format
   ```

2. **Standardization**
   ```bash
   cd movie_estandarizer
   python estandarizer.py --all-records
   ```

3. **Enrichment**
   ```bash
   python movie_enricher.py
   # Or run complete pipeline:
   python complete_pipeline.py
   ```

4. **Analysis**
   ```sql
   -- Run order range recognition queries
   -- Analyze payment patterns
   ```

---

## üìà Performance Metrics

### Movie Standardizer
- **Input**: 17M+ records (5GB)
- **Output**: ~2,000 unique movies
- **Processing**: ~200,000 records/second
- **Memory**: 4GB RAM max
- **Time**: 15-20 minutes total

### Web Scraping
- **Speed**: ~50 movies/minute
- **Success Rate**: ~80% extraction
- **Retry Logic**: Automatic on failures

---

## üîç Key Algorithms

### Movie Deduplication
```python
# Multi-level normalization
Raw Name ‚Üí TITULO_LIMPIO ‚Üí NOMBRE_UNICO ‚Üí FAMILIA
"Superman 4DX Esp" ‚Üí "SUPERMAN 4DX ESP" ‚Üí "SUPERMAN" ‚Üí "SUPERMAN"
```

### Bayesian Binning
```sql
-- Knuth's Bayesian optimization
-- Finds optimal bin count M*
-- Maximizes marginal likelihood
-- No arbitrary parameters needed
```

---

## üõ°Ô∏è Error Handling

### Missing Input Files
```bash
‚ùå ERROR: Input file not found!
Please follow these steps:
1. Execute the SQL query
2. Export results to CSV
3. Save to appropriate location
```

### Automatic Recovery
- Creates missing directories
- Generates empty templates
- Continues on partial failures
- Logs all errors for review

---

## üìù Logging

All modules generate detailed logs:
- `movie_standardizer.log`
- `movie_enricher.log`
- `complete_pipeline.log`